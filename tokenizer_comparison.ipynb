{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de855d1c-77c7-445d-b258-b7430f3ad631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\FPTSHOP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimeit\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from transformers import BertTokenizer\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d87476e-ece0-480a-ace4-73a843666379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer for spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4367f89b-6065-442f-ab40-d6b9dbbc300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db2230c9-c254-4e5a-9e10-4c80def39023",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b6ad0f-292c-4c5c-810c-890328457792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2dc7c75e-9ea7-4186-9703-71396f09fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(text):\n",
    "    word_tokenize_tokens = word_tokenize(text)\n",
    "    tweet_tokens = tweet.tokenize(text)\n",
    "    treebank_tokens = treebank.tokenize(text)\n",
    "    bert_tokens = bert.tokenize(text)\n",
    "    spacy_tokens = [i.text for i in nlp(text)]\n",
    "    # Measure tokenization time\n",
    "    word_tokenize_tokens_time = timeit.timeit(lambda: word_tokenize(text), number=1000)\n",
    "    tweet_tokens_time = timeit.timeit(lambda: tweet.tokenize(text), number=1000)\n",
    "    treebank_tokens_time = timeit.timeit(lambda: treebank.tokenize(text), number=1000)\n",
    "    bert_tokens_time = timeit.timeit(lambda: bert.tokenize(text), number=1000)\n",
    "    spacy_tokens_time = timeit.timeit(lambda: [i.text for i in nlp(text)], number=1000)\n",
    "    print(f\"Word_Tokenize: {word_tokenize_tokens} | time: {word_tokenize_tokens_time}\")\n",
    "    print(f\"TweetTokenizer: {tweet_tokens}  | time: {tweet_tokens_time}\")\n",
    "    print(f\"TreebankWordTokenizer: {treebank_tokens}  | time: {treebank_tokens_time}\")\n",
    "    print(f\"BertTokenizer: {bert_tokens}  | time: {bert_tokens_time}\")\n",
    "    print(f\"Spacy (en_core_web_sm): {spacy_tokens} | time: {spacy_tokens_time}\")\n",
    "    print(f\"Fastest Time: {min(word_tokenize_tokens_time, tweet_tokens_time, treebank_tokens_time, bert_tokens_time, spacy_tokens_time)}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bea1cf63-2ba4-48f0-996c-55fc9d02b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Tokenize: ['GoodMorning'] | time: 0.03265140001894906\n",
      "TweetTokenizer: ['GoodMorning']  | time: 0.020079600013559684\n",
      "TreebankWordTokenizer: ['GoodMorning']  | time: 0.022092399973189458\n",
      "BertTokenizer: ['good', '##mo', '##rn', '##ing']  | time: 0.10094299999764189\n",
      "Spacy (en_core_web_sm): ['GoodMorning'] | time: 7.877144199999748\n",
      "Fastest Time: 0.020079600013559684\n"
     ]
    }
   ],
   "source": [
    "comparison(\"GoodMorning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d197084-b330-4c5e-9f5b-6edaf751d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Tokenize: ['I', \"'ve\", 'never', 'seen', 'this', 'before', '!'] | time: 0.05664039999828674\n",
      "TweetTokenizer: [\"I've\", 'never', 'seen', 'this', 'before', '!']  | time: 0.042168799991486594\n",
      "TreebankWordTokenizer: ['I', \"'ve\", 'never', 'seen', 'this', 'before', '!']  | time: 0.028735400002915412\n",
      "BertTokenizer: ['i', \"'\", 've', 'never', 'seen', 'this', 'before', '!']  | time: 0.1752991999965161\n",
      "Spacy (en_core_web_sm): ['I', \"'ve\", 'never', 'seen', 'this', 'before', '!'] | time: 10.270850200002315\n",
      "Fastest Time: 0.028735400002915412\n"
     ]
    }
   ],
   "source": [
    "comparison(\"I've never seen this before!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7759dfb1-8b46-4e1f-8e61-92da61ef25dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Tokenize: ['Dr.', 'Smith', 'bought', '1,000', 'shares', 'of', 'Acme', 'Corp.', 'on', 'Jan.', '5th', ',', '2023', '!', 'He', 'said', ':', \"'\", 'I', \"'m\", 'optimistic', 'about', 'the', 'company', \"'s\", 'growth—especially', 'in', 'AI', 'and', 'ML', '.', \"'\", 'Meanwhile', ',', 'competitors', 'are', 'struggling', 'with', 'COVID-19', 'related', 'delays', '.'] | time: 0.41654840001137927\n",
      "TweetTokenizer: ['Dr', '.', 'Smith', 'bought', '1,000', 'shares', 'of', 'Acme', 'Corp', '.', 'on', 'Jan', '.', '5th', ',', '2023', '!', 'He', 'said', ':', \"'\", \"I'm\", 'optimistic', 'about', 'the', \"company's\", 'growth', '—', 'especially', 'in', 'AI', 'and', 'ML', '.', \"'\", 'Meanwhile', ',', 'competitors', 'are', 'struggling', 'with', 'COVID', '-', '19', 'related', 'delays', '.']  | time: 0.3306933999992907\n",
      "TreebankWordTokenizer: ['Dr.', 'Smith', 'bought', '1,000', 'shares', 'of', 'Acme', 'Corp.', 'on', 'Jan.', '5th', ',', '2023', '!', 'He', 'said', ':', \"'I\", \"'m\", 'optimistic', 'about', 'the', 'company', \"'s\", 'growth—especially', 'in', 'AI', 'and', 'ML.', \"'\", 'Meanwhile', ',', 'competitors', 'are', 'struggling', 'with', 'COVID-19', 'related', 'delays', '.']  | time: 0.14113259999430738\n",
      "BertTokenizer: ['dr', '.', 'smith', 'bought', '1', ',', '000', 'shares', 'of', 'ac', '##me', 'corp', '.', 'on', 'jan', '.', '5th', ',', '202', '##3', '!', 'he', 'said', ':', \"'\", 'i', \"'\", 'm', 'optimistic', 'about', 'the', 'company', \"'\", 's', 'growth', '—', 'especially', 'in', 'ai', 'and', 'ml', '.', \"'\", 'meanwhile', ',', 'competitors', 'are', 'struggling', 'with', 'co', '##vid', '-', '19', 'related', 'delays', '.']  | time: 0.8794543000112753\n",
      "Spacy (en_core_web_sm): ['Dr.', 'Smith', 'bought', '1,000', 'shares', 'of', 'Acme', 'Corp.', 'on', 'Jan.', '5th', ',', '2023', '!', 'He', 'said', ':', \"'\", 'I', \"'m\", 'optimistic', 'about', 'the', 'company', \"'s\", 'growth', '—', 'especially', 'in', 'AI', 'and', 'ML', '.', \"'\", 'Meanwhile', ',', 'competitors', 'are', 'struggling', 'with', 'COVID-19', 'related', 'delays', '.'] | time: 21.200922499992885\n",
      "Fastest Time: 0.14113259999430738\n"
     ]
    }
   ],
   "source": [
    "comparison(\"Dr. Smith bought 1,000 shares of Acme Corp. on Jan. 5th, 2023! He said: 'I'm optimistic about the company's growth—especially in AI and ML.' Meanwhile, competitors are struggling with COVID-19 related delays.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379b815-4f53-4a80-aa20-ad2870847b04",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer is the fastest tokenizer, and Spacy is the lowest (much lower than other tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd30380-f832-4c20-b981-980d1c03a9bc",
   "metadata": {},
   "source": [
    "SpaCy does a lot more than just tokenize (POS tagging,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9afa97-1e7c-4521-9f2d-94d99de7ffe1",
   "metadata": {},
   "source": [
    "Bert breaks words into subwords -> important for DL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d58a76-d86f-434b-9ce0-0389b114bf7e",
   "metadata": {},
   "source": [
    "Tweet works well for social media text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6871af-8330-4c42-8d8e-a0adde4fecc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizer_comparison_env",
   "language": "python",
   "name": "tokenizer_comparison_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
